{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import shutil\n",
        "import os\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "import lightning as L\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "\n",
        "file_id = '1YHaxS8f6fQ5IMT3JJVeBH40eZSS-UC9h'\n",
        "zip_file_path = '/content/Images.zip'\n",
        "extract_folder = '/content/Images'\n",
        "\n",
        "# Download and extract only if not already done\n",
        "if not os.path.exists(extract_folder):\n",
        "    gdown.download(f'https://drive.google.com/uc?export=download&id={file_id}', zip_file_path, quiet=False)\n",
        "    shutil.unpack_archive(zip_file_path, extract_folder)\n",
        "    print(\"Files extracted:\", os.listdir(extract_folder))\n",
        "else:\n",
        "    print(\"Files already extracted.\")\n",
        "\n",
        "path = Path(extract_folder) / 'Images'"
      ],
      "metadata": {
        "id": "57nuxTPrBE68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastai.vision.all import *\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Paths\n",
        "\n",
        "#path = Path(\"path/to/original/images\")  # Replace with the path to your original images\n",
        "save_path = Path(\"cropped/images\")  # Replace with the path to save cropped images\n",
        "save_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Function to crop images into overlapping tiles\n",
        "def crop_with_overlap(image_path, save_path, tile_size=(512, 512), stride=(256, 256)):\n",
        "    img = Image.open(image_path)\n",
        "    width, height = img.size\n",
        "    label = image_path.parent.name  # Extract label from parent folder\n",
        "    class_save_path = save_path / label\n",
        "    class_save_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    for i in range(0, width - tile_size[0] + 1, stride[0]):\n",
        "        for j in range(0, height - tile_size[1] + 1, stride[1]):\n",
        "            left = i\n",
        "            upper = j\n",
        "            right = left + tile_size[0]\n",
        "            lower = upper + tile_size[1]\n",
        "            tile = img.crop((left, upper, right, lower))\n",
        "            tile.save(class_save_path / f\"{image_path.stem}_tile_{i}_{j}.jpg\")\n",
        "\n",
        "# Crop images into overlapping tiles\n",
        "tile_size = (512, 512)\n",
        "stride = (256, 256)\n",
        "\n",
        "image_files = get_image_files(path)\n",
        "for image_file in image_files:\n",
        "    crop_with_overlap(image_file, save_path, tile_size=tile_size, stride=stride)\n",
        "\n",
        "# Verify cropped images\n",
        "sliced_image_files = get_image_files(save_path)\n",
        "print(f\"Total cropped images: {len(sliced_image_files)}\")"
      ],
      "metadata": {
        "id": "TojkhaPhB9_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3vSygvuA9WJ"
      },
      "outputs": [],
      "source": [
        "from fastai.vision.all import *\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Define the VAE model\n",
        "# This class defines a Variational Autoencoder (VAE) for 512x512 images.\n",
        "class VAE(nn.Module):\n",
        "    def __init__(self, img_size=512, latent_dim=512):\n",
        "        \"\"\"\n",
        "        Initialize the VAE model with:\n",
        "        - img_size: Dimensions of the input image (e.g., 512x512).\n",
        "        - latent_dim: The size of the latent space, where the image is encoded.\n",
        "        \"\"\"\n",
        "        super(VAE, self).__init__()\n",
        "        self.img_size = img_size\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Encoder: Compresses the image into a latent representation\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, 4, 2, 1),   # B, 64, img_size/2, img_size/2\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 4, 2, 1), # B, 128, img_size/4, img_size/4\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(128, 256, 4, 2, 1), # B, 256, img_size/8, img_size/8\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(256, 512, 4, 2, 1), # B, 512, img_size/16, img_size/16\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "        # Two fully connected layers for the latent space:\n",
        "        # fc_mu predicts the mean, and fc_logvar predicts the log of the variance.\n",
        "        self.fc_mu = nn.Linear(512 * (img_size // 16) ** 2, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(512 * (img_size // 16) ** 2, latent_dim)\n",
        "\n",
        "        # Decoder: Reconstructs the image from the latent representation\n",
        "        self.fc_decode = nn.Linear(latent_dim, 512 * (img_size // 16) ** 2)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(256, 128, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 3, 4, 2, 1),\n",
        "            nn.Sigmoid()  # Outputs values between 0 and 1\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"\n",
        "        Encodes the input image into the latent space by generating the mean (mu)\n",
        "        and the log variance (logvar).\n",
        "        \"\"\"\n",
        "        x = self.encoder(x)\n",
        "        mu, logvar = self.fc_mu(x), self.fc_logvar(x)\n",
        "        return mu, logvar\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        \"\"\"\n",
        "        Reparameterization trick to sample from a normal distribution using\n",
        "        the predicted mean and variance.\n",
        "        \"\"\"\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)  # Random noise\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"\n",
        "        Decodes the latent vector z back into an image.\n",
        "        \"\"\"\n",
        "        z = self.fc_decode(z).view(-1, 512, self.img_size // 16, self.img_size // 16)\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Full forward pass: encodes the image, samples from the latent space,\n",
        "        and reconstructs the image.\n",
        "        \"\"\"\n",
        "        mu, logvar = self.encode(x)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar\n",
        "\n",
        "# Step 2: Define custom VAE loss function\n",
        "# This combines reconstruction loss (how well the image is reconstructed)\n",
        "# and KL divergence (how close the latent distribution is to a standard normal distribution).\n",
        "def vae_loss_fn(preds, x, mu, logvar):\n",
        "    recon_x, _, _ = preds\n",
        "    # Reconstruction loss: measures pixel-wise difference\n",
        "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
        "    # KL divergence: encourages latent space to follow a normal distribution\n",
        "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "    return recon_loss + kld_loss\n",
        "\n",
        "# Step 3: Prepare the data using Fastai's DataBlock\n",
        "# Define the structure of the dataset and transformations.\n",
        "data_path = Path(\"cropped/images/\")  # Path to your dataset\n",
        "dblock = DataBlock(\n",
        "    blocks=(ImageBlock, ImageBlock),  # Input and output both are images\n",
        "    get_items=get_image_files,        # Function to retrieve all image files\n",
        "    splitter=RandomSplitter(),        # Split into training and validation sets\n",
        "    item_tfms=Resize(512),            # Resize all images to 512x512\n",
        "    batch_tfms=Normalize.from_stats(*imagenet_stats)  # Normalize using ImageNet statistics\n",
        ")\n",
        "\n",
        "# Create DataLoaders for training and validation\n",
        "dls = dblock.dataloaders(data_path, bs=8)  # Use smaller batch size for large images\n",
        "\n",
        "# Step 4: Train the VAE model\n",
        "# Initialize the VAE model\n",
        "vae = VAE(img_size=512, latent_dim=512)\n",
        "\n",
        "# Use Fastai's Learner to manage training\n",
        "learn = Learner(dls, vae, loss_func=vae_loss_fn, metrics=[])\n",
        "\n",
        "# Fit the model for 10 epochs\n",
        "learn.fit(10, lr=1e-3)\n",
        "\n",
        "# Step 5: Generate new images by sampling from the latent space\n",
        "vae.eval()  # Switch to evaluation mode\n",
        "with torch.no_grad():\n",
        "    # Sample a random point in the latent space\n",
        "    z = torch.randn(1, 512)  # Latent space size = 512\n",
        "    # Decode the latent vector to generate an image\n",
        "    generated_img = vae.decode(z).squeeze().permute(1, 2, 0)  # Reshape for visualization\n",
        "    # Convert to a displayable format (0-255 pixel range)\n",
        "    generated_img = (generated_img * 255).numpy().astype(\"uint8\")\n",
        "\n",
        "# Step 6: Visualize the generated image\n",
        "# Display the generated image\n",
        "plt.imshow(generated_img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save both model and optimizer state_dict\n",
        "torch.save({\n",
        "    'model_state_dict': vae.state_dict(),\n",
        "    'optimizer_state_dict': learn.opt.state_dict()\n",
        "}, \"vae_model_and_optimizer.pth\")\n",
        "\n",
        "# Load the saved states\n",
        "checkpoint = torch.load(\"vae_model_and_optimizer.pth\")\n",
        "loaded_vae.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ],
      "metadata": {
        "id": "0uKW6DWkFQBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference example\n",
        "with torch.no_grad():\n",
        "    z = torch.randn(1, 512)  # Sample from the latent space\n",
        "    generated_img = loaded_vae.decode(z).squeeze().permute(1, 2, 0)\n",
        "    generated_img = (generated_img * 255).numpy().astype(\"uint8\")\n",
        "\n",
        "# Visualize the generated image\n",
        "plt.imshow(generated_img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wTirv_pbFVnE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}